
note : feature is a input variables or columns 

Feature engineering is the process of selecting, extracting, and transforming the input data to create new features that can be used as inputs to a machine learning algorithm. The goal of feature engineering is to improve the performance of a machine learning model by providing it with more relevant and informative features


 1 ) Feature Transformation: Feature transformation involves transforming the original features into a new set of features using mathematical transformations such as log transformation, square root transformation, and Box-Cox transformation.

 2 ) Feature Construction: sometimes their wont be a desired colunns which we want which we think if added then it impove the ml model so we need to add those feature / columns
it  nvolves creating new features by combining two or more existing features. Common techniques include arithmetic operations, polynomial features, and interaction features.

  3 ) Feature Scaling: Feature scaling involves scaling the numerical features to a similar range. Common techniques include standardization, min-max scaling, and robust scaling.

4)  Feature Extraction: Feature extraction involves selecting a subset of the original features that are most relevant to the problem at hand. Common techniques include principal component analysis (PCA), linear discriminant analysis (LDA), and t-distributed stochastic neighbor embedding (t-SNE).

 5)  Time-Series Features: Time-series data requires specific feature engineering techniques such as lag features, rolling statistics, and exponential smoothing.
